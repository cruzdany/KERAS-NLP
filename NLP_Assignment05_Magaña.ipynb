{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment05_Magaña.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cruzdany/KERAS-NLP/blob/main/NLP_Assignment05_Maga%C3%B1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slNXYz304EqL"
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from numpy import *  \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqw6MJrF4EqO"
      },
      "source": [
        "df = pd.read_csv('train.csv',encoding='utf-8',error_bad_lines=False, engine=\"python\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Za0XIzXvCwK",
        "outputId": "6ee7c15c-f452-4c67-cade-f84122690328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "\"\"\"import nltk\n",
        "nltk.download('stopwords')\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"import nltk\\nnltk.download('stopwords')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "0o3TzCgnu-C9",
        "outputId": "19aaf495-f810-43c6-de6f-a953dc10c8f3"
      },
      "source": [
        "\"\"\"from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "def scrub_words(text):\n",
        "    # remove html markup\n",
        "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
        "    \n",
        "    #remove non-ascii and digits\n",
        "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
        "  \n",
        "    #remove whitespace\n",
        "    text=text.strip()\n",
        "    return text\n",
        "  \n",
        "#Noise removal, stop word removal, normalizing?\n",
        "def cleanString(s, special_chars = \"\\\":,.@|ðÿœžðÿâœœïÿœžÿºÿÿœžÿ=_\"):\n",
        "    for char in special_chars:\n",
        "        s = s.replace(char, \"\")\n",
        "    s = s.replace(\"\\n\", \"\")\n",
        "    s = scrub_words(s)\n",
        "    tokenizer = TweetTokenizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_words = [w for w in tokenizer.tokenize(s) if w not in stop_words]\n",
        "    return \" \".join(cleaned_words), tokenizer\n",
        "\n",
        "def stemWords(sentence):\n",
        "    stemmer, tokenizer = PorterStemmer(), TweetTokenizer()\n",
        "    stemmed_words = [stemmer.stem(w) for w in tokenizer.tokenize(sentence)]\n",
        "    return \" \".join(stemmed_words)\n",
        "    \n",
        "def cleanFrame(frame):\n",
        "    frame['clean_tweet'] = frame.comment_text.apply(cleanString)\n",
        "\n",
        "\n",
        "cleanFrame(df)\n",
        "df.head()\"\"\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from nltk.corpus import stopwords\\nfrom nltk.tokenize import TweetTokenizer\\nfrom nltk.stem import PorterStemmer\\nimport re\\n\\ndef scrub_words(text):\\n    # remove html markup\\n    text=re.sub(\"(<.*?>)\",\"\",text)\\n    \\n    #remove non-ascii and digits\\n    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\\n  \\n    #remove whitespace\\n    text=text.strip()\\n    return text\\n  \\n#Noise removal, stop word removal, normalizing?\\ndef cleanString(s, special_chars = \"\":,.@|ðÿœžðÿâœœïÿœžÿºÿÿœžÿ=_\"):\\n    for char in special_chars:\\n        s = s.replace(char, \"\")\\n    s = s.replace(\"\\n\", \"\")\\n    s = scrub_words(s)\\n    tokenizer = TweetTokenizer()\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    cleaned_words = [w for w in tokenizer.tokenize(s) if w not in stop_words]\\n    return \" \".join(cleaned_words), tokenizer\\n\\ndef stemWords(sentence):\\n    stemmer, tokenizer = PorterStemmer(), TweetTokenizer()\\n    stemmed_words = [stemmer.stem(w) for w in tokenizer.tokenize(sentence)]\\n    return \" \".join(stemmed_words)\\n    \\ndef cleanFrame(frame):\\n    frame[\\'clean_tweet\\'] = frame.comment_text.apply(cleanString)\\n\\n\\ncleanFrame(df)\\ndf.head()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3dopVRd4EqU"
      },
      "source": [
        "Import Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "AmVxgTg9zzNy",
        "outputId": "165a7477-d362-4335-e615-08deaa8ab5c3"
      },
      "source": [
        "import re\n",
        "special_chars = r\"[^0-9!@#\\$%\\^\\&\\*_\\-']\"\n",
        "def Cleaning(tweet,special_chars):\n",
        "    tweet = re.sub(\"\\d+\", \"\", tweet)\n",
        "    tweet=re.sub(\"h[\\S]+//[\\S]+\", \"\", tweet)\n",
        "    tweet=re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", tweet)\n",
        "    tweet=tweet.lower()\n",
        "    for char in special_chars:\n",
        "        tweet = tweet.replace(char, \"\")    \n",
        "    tweet=tweet.split(\" \")\n",
        "    return ' '.join(tweet)\n",
        "Cleaned=[]\n",
        "for t in df[\"comment_text\"]:\n",
        "    Cleaned.append(Cleaning(t,special_chars))\n",
        "df['text_generation'] = Cleaned\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>text_generation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>explanationwhy the edits made under my usernam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>daww he matches this background colour im seem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hey man im really not trying to edit war its j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>morei cant make any real suggestions on improv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ...                                    text_generation\n",
              "0  0000997932d777bf  ...  explanationwhy the edits made under my usernam...\n",
              "1  000103f0d9cfb60f  ...  daww he matches this background colour im seem...\n",
              "2  000113f07ec002fd  ...  hey man im really not trying to edit war its j...\n",
              "3  0001b41b1c6bb37e  ...  morei cant make any real suggestions on improv...\n",
              "4  0001d958c54c6e35  ...  you sir are my hero any chance you remember wh...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_kLJ9_s4EqW",
        "outputId": "26c91984-14a1-405e-e6d2-5c9eb56b73d6"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
              "       'insult', 'identity_hate', 'text_generation'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-CSUJ-s6DFw"
      },
      "source": [
        "y = df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfNHXInq4EqX",
        "outputId": "33a9852e-c560-4e80-e1cf-81ab61e39899"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.text_generation, y, test_size=0.2,\n",
        "                                                    random_state=np.random)\n",
        "print(X_train.shape, X_test.shape, len(y_train), len(y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(127656,) (31915,) 127656 31915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFoxaUSY4EqY",
        "outputId": "d2d2590f-981b-47b1-a1b5-2bda459c99e9"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "max_features = 20000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
        "maxlen=400\n",
        "max_features = 20000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
        "V_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "V_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "print(V_train.shape,V_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(127656, 400) (31915, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_k5EWJ84EqZ",
        "outputId": "2c9a803c-81b3-4168-94b6-946ac4403cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "\"\"\"def getModel():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    embed_size = 128\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = LSTM(30, return_sequences=True,name='lstm_layer')(x)\n",
        "    #x = GlobalMaxPool1D()(x)\n",
        "    #x = Dropout(0.5)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    #x = Dropout(0.5)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    x = \n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def getModel():\\n    inp = Input(shape=(maxlen, ))\\n    embed_size = 128\\n    x = Embedding(max_features, embed_size)(inp)\\n    x = LSTM(30, return_sequences=True,name=\\'lstm_layer\\')(x)\\n    #x = GlobalMaxPool1D()(x)\\n    #x = Dropout(0.5)(x)\\n    x = Dense(20, activation=\"relu\")(x)\\n    #x = Dropout(0.5)(x)\\n    x = Dense(6, activation=\"sigmoid\")(x)\\n    x = \\n    model = Model(inputs=inp, outputs=x)\\n    model.compile(loss=\\'binary_crossentropy\\',\\n                      optimizer=\\'adam\\',\\n                      metrics=[\\'accuracy\\'])\\n    return model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8USoidMUPpU"
      },
      "source": [
        "def getModel():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    embed_size = 128\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = LSTM(30, return_sequences=True,name='lstm_layer')(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYUhlX78HvuW",
        "outputId": "31ae2401-5e70-4c83-93c2-da93d10167fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "\"\"\"def getModel():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    embed_size = 128\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = LSTM(30, return_sequences=True,name='lstm_layer')(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def getModel():\\n    inp = Input(shape=(maxlen, ))\\n    embed_size = 128\\n    x = Embedding(max_features, embed_size)(inp)\\n    x = LSTM(30, return_sequences=True,name=\\'lstm_layer\\')(x)\\n    x = GlobalMaxPool1D()(x)\\n    x = Dropout(0.5)(x)\\n    x = Dense(20, activation=\"relu\")(x)\\n    x = Dropout(0.5)(x)\\n    x = Dense(6, activation=\"sigmoid\")(x)\\n    model = Model(inputs=inp, outputs=x)\\n    model.compile(loss=\\'binary_crossentropy\\',\\n                      optimizer=\\'adam\\',\\n                      metrics=[\\'accuracy\\'])\\n    return model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A6StChbAW4r",
        "outputId": "87a2fe63-de3c-461c-e22b-baabdd590930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "\"\"\"def getModel():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    embed_size = 128\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = LSTM(30, return_sequences=True,name='lstm_layer')(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(6, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def getModel():\\n    inp = Input(shape=(maxlen, ))\\n    embed_size = 128\\n    x = Embedding(max_features, embed_size)(inp)\\n    x = LSTM(30, return_sequences=True,name=\\'lstm_layer\\')(x)\\n    x = GlobalMaxPool1D()(x)\\n    x = Dropout(0.5)(x)\\n    x = Dense(20, activation=\"relu\")(x)\\n    x = Dropout(0.5)(x)\\n    x = Dense(6, activation=\"softmax\")(x)\\n    model = Model(inputs=inp, outputs=x)\\n    model.compile(loss=\\'binary_crossentropy\\',\\n                      optimizer=\\'adam\\',\\n                      metrics=[\\'accuracy\\'])\\n    return model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mabMuv8c4EqZ",
        "outputId": "45ac13df-ee20-45d0-fb93-3044955953e8"
      },
      "source": [
        "model = getModel()\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "file_path=\"../weights_base.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=2)\n",
        "\n",
        "\n",
        "callbacks_list = [checkpoint, early] #early\n",
        "model.fit(V_train,y_train,  batch_size=batch_size, epochs=epochs, \n",
        "          validation_split=0.1, callbacks=callbacks_list)\n",
        "\n",
        "model.load_weights(file_path)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3591/3591 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.8948\n",
            "Epoch 00001: val_loss improved from inf to 0.05058, saving model to ../weights_base.best.hdf5\n",
            "3591/3591 [==============================] - 737s 205ms/step - loss: 0.0803 - accuracy: 0.8948 - val_loss: 0.0506 - val_accuracy: 0.9941\n",
            "Epoch 2/3\n",
            "3591/3591 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9886\n",
            "Epoch 00002: val_loss did not improve from 0.05058\n",
            "3591/3591 [==============================] - 744s 207ms/step - loss: 0.0498 - accuracy: 0.9886 - val_loss: 0.0506 - val_accuracy: 0.9941\n",
            "Epoch 3/3\n",
            "3591/3591 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9858\n",
            "Epoch 00003: val_loss improved from 0.05058 to 0.04939, saving model to ../weights_base.best.hdf5\n",
            "3591/3591 [==============================] - 739s 206ms/step - loss: 0.0450 - accuracy: 0.9858 - val_loss: 0.0494 - val_accuracy: 0.9941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1XXy-5R4EqZ",
        "outputId": "c77564b8-b807-4fe7-9e13-5a0a9cb8b0af"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "y_pred = model.predict(V_test)\n",
        "print(classification_report(y_test,y_pred.round(),digits=6))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.812227  0.767132  0.789036      3152\n",
            "           1   0.857143  0.017804  0.034884       337\n",
            "           2   0.831938  0.780584  0.805443      1782\n",
            "           3   0.000000  0.000000  0.000000        95\n",
            "           4   0.721178  0.697999  0.709399      1649\n",
            "           5   0.000000  0.000000  0.000000       311\n",
            "\n",
            "   micro avg   0.794306  0.677860  0.731477      7326\n",
            "   macro avg   0.537081  0.377253  0.389794      7326\n",
            "weighted avg   0.753581  0.677860  0.696682      7326\n",
            " samples avg   0.069346  0.065508  0.064548      7326\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQyw7abaFQyn"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKy6WcM4IKR3"
      },
      "source": [
        "#y_pred = (y_pred > 0.5) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "UbsElobtHt42",
        "outputId": "f116b893-3854-47ad-86ab-65cf5449bf92"
      },
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-430e012b2078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYuD0OVwSQSc"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhSYSun4SR1L"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rQF_WJuITjd"
      },
      "source": [
        "confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAVMZ37OMGcy"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hra3KUhzMIS9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}